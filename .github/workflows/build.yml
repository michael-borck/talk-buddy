name: Build and Release

on:
  push:
    tags:
      - 'v*'

permissions:
  contents: write

jobs:
  build:
    runs-on: ${{ matrix.os }}
    
    strategy:
      matrix:
        os: [macos-latest, ubuntu-latest, windows-latest]
        include:
          - os: macos-latest
            platform: mac
          - os: ubuntu-latest
            platform: linux
          - os: windows-latest
            platform: win

    steps:
      - name: Check out Git repository
        uses: actions/checkout@v4

      - name: Install Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install dependencies
        run: npm ci

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Build embedded server (Unix)
        if: matrix.os != 'windows-latest'
        run: |
          cd embedded-server
          python -m pip install --upgrade pip
          # Install lightweight pywhispercpp instead of openai-whisper (saves ~1.6GB)
          pip install piper-tts flask flask-cors pywhispercpp pyinstaller
          
          # Download Piper voice models
          mkdir -p models
          curl -L -o models/en_GB-alan-low.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_GB/alan/low/en_GB-alan-low.onnx
          curl -L -o models/en_GB-alan-low.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_GB/alan/low/en_GB-alan-low.onnx.json
          curl -L -o models/en_US-amy-low.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/low/en_US-amy-low.onnx
          curl -L -o models/en_US-amy-low.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/low/en_US-amy-low.onnx.json
          
          # Download whisper-tiny model for pywhispercpp (will be included in build)
          mkdir -p whisper-models
          curl -L -o whisper-models/ggml-tiny.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin
          
          # Create PyInstaller executable with optimizations
          pyinstaller --onefile \
            --add-data "models:models" \
            --add-data "whisper-models:whisper-models" \
            --exclude-module torch \
            --exclude-module tensorflow \
            --exclude-module PIL \
            --exclude-module matplotlib \
            --exclude-module pandas \
            --exclude-module scipy \
            --strip \
            --name embedded-server server.py
          
          # Copy executable to main project for Electron to bundle
          mkdir -p ../dist-server
          cp dist/embedded-server* ../dist-server/

      - name: Build embedded server (Windows)
        if: matrix.os == 'windows-latest'
        run: |
          cd embedded-server
          python -m pip install --upgrade pip
          pip install piper-tts flask flask-cors pywhispercpp pyinstaller
          
          mkdir models
          curl -L -o models/en_GB-alan-low.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_GB/alan/low/en_GB-alan-low.onnx
          curl -L -o models/en_GB-alan-low.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_GB/alan/low/en_GB-alan-low.onnx.json
          curl -L -o models/en_US-amy-low.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/low/en_US-amy-low.onnx
          curl -L -o models/en_US-amy-low.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/amy/low/en_US-amy-low.onnx.json
          
          mkdir whisper-models
          curl -L -o whisper-models/ggml-tiny.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin
          
          pyinstaller --onefile --add-data "models;models" --add-data "whisper-models;whisper-models" --exclude-module torch --exclude-module tensorflow --exclude-module PIL --exclude-module matplotlib --exclude-module pandas --exclude-module scipy --name embedded-server server.py
          
          mkdir ..\dist-server
          copy dist\embedded-server* ..\dist-server\

      - name: Build Vite app
        run: npm run build

      - name: Build Electron app (Windows)
        if: matrix.os == 'windows-latest'
        run: npm run electron:build -- --win --publish=always
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Build Electron app (macOS)
        if: matrix.os == 'macos-latest'
        run: npm run electron:build -- --mac --publish=always
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          CSC_IDENTITY_AUTO_DISCOVERY: false

      - name: Free disk space (Linux)
        if: matrix.os == 'ubuntu-latest'
        run: |
          echo "Available space before cleanup:"
          df -h
          # Remove unnecessary packages and files to free space
          sudo apt-get clean
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          docker system prune -af
          # Clean up embedded server build artifacts (keeping dist-server for Electron)
          rm -rf embedded-server/build embedded-server/__pycache__
          find embedded-server -name "*.pyc" -delete 2>/dev/null || true
          # Clean up large Python cache files
          pip cache purge 2>/dev/null || true
          echo "Available space after cleanup:"
          df -h

      - name: Build Electron app (Linux)
        if: matrix.os == 'ubuntu-latest'
        run: npm run electron:build -- --linux --publish=always
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.platform }}-distributables
          path: |
            dist/*.exe
            dist/*.msi
            dist/*.dmg
            dist/*.pkg
            dist/*.AppImage
            dist/*.deb
            dist/*.snap
            dist/*.yml
            dist/*.yaml

